{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d138eaae-f1fb-4b0a-97aa-7959b072b103",
   "metadata": {},
   "source": [
    "Assignment Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ef389-e237-4785-9a8a-1fa72b7bbee5",
   "metadata": {},
   "source": [
    "Question 1: Compare and contrast NLTK and spaCy in terms of features, ease of use,and performance.\n",
    "Answer: Primary Goal/Design\n",
    "NLTK: Developed primarily for education, research, and prototyping. It functions as a comprehensive \"toolbox\" that provides a wide array of algorithms and resources for exploring computational linguistics.\n",
    "\n",
    "spaCy: Designed for production use, speed, and efficiency. It offers a streamlined NLP \"service\" that gets specific, complex tasks done quickly in real-world applications.\n",
    "\n",
    "Implementation and Performance\n",
    "NLTK:\n",
    "\n",
    "Implementation: Primarily Pure Python, which, while flexible, leads to slower processing times.\n",
    "\n",
    "Performance: Generally slower, especially when processing large volumes of text, due to its Python implementation and design focus on completeness over raw speed.\n",
    "\n",
    "spaCy:\n",
    "\n",
    "Implementation: Highly optimized using Cython (a C-extension for Python), enabling near-native execution speeds.\n",
    "\n",
    "Performance: Significantly Faster than NLTK. It is optimized for minimal memory overhead and high throughput, making it ideal for large-scale and real-time systems.\n",
    "\n",
    "Core Features and Models\n",
    "NLTK:\n",
    "\n",
    "Offers a vast collection of algorithms for tokenization, stemming, tagging, parsing, classification, etc.\n",
    "\n",
    "Provides access to over 50 corpora and lexical resources like WordNet, making it comprehensive for linguistic analysis.\n",
    "\n",
    "Primarily a string processing library (functions take and return strings/lists).\n",
    "\n",
    "spaCy:\n",
    "\n",
    "Provides highly efficient, state-of-the-art statistical models for core tasks like Tokenization, Part-of-Speech Tagging (POS), Named Entity Recognition (NER), and Dependency Parsing.\n",
    "\n",
    "An object-oriented library that returns structured Doc objects with annotations, making pipeline integration seamless.\n",
    "\n",
    "Lacks built-in stemming (it focuses on the more accurate lemmatization).\n",
    "\n",
    "Algorithmic Choice and Flexibility\n",
    "NLTK:\n",
    "\n",
    "High Flexibility: Developers have the freedom to choose between multiple algorithms for the same task (e.g., various tokenizers, or stemmers like Porter or Snowball).\n",
    "\n",
    "Great for rule-based NLP and custom implementations built from scratch.\n",
    "\n",
    "spaCy:\n",
    "\n",
    "Minimal Choice: It generally provides the single, best, and most efficient available algorithm for each task, prioritizing production-grade accuracy and speed over experimental flexibility.\n",
    "\n",
    "Designed as a pipeline, where components run in sequence, making it less customizable at the fundamental algorithm level.\n",
    "\n",
    "Ease of Use and Learning Curve\n",
    "NLTK:\n",
    "\n",
    "Steeper Learning Curve: Requires more explicit code for basic tasks and can be overwhelming for beginners due to the sheer number of available modules, algorithms, and corpora.\n",
    "\n",
    "Excellent community and educational resources (like the official NLTK book).\n",
    "\n",
    "spaCy:\n",
    "\n",
    "User-Friendly API: Features a simple, intuitive, and object-oriented API.\n",
    "\n",
    "Provides pre-trained models that can be loaded in one line of code, enabling quick setup for production-ready tasks.\n",
    " Multi-Language Support\n",
    "NLTK:\n",
    "\n",
    "Broader Support: Supports a very wide range of languages, often relying on language-specific resources and corpora that a user must download.\n",
    "\n",
    "spaCy:\n",
    "\n",
    "Targeted Support: Focuses on having fast and robust trained pipelines for key, high-demand languages (e.g., trained pipelines for over 25 languages), and basic support for 75+ languages.\n",
    "\n",
    "\n",
    "Question 2: What is TextBlob and how does it simplify common NLP tasks like\n",
    "sentiment analysis and translation?\n",
    "Answer:  Core Concept\n",
    "TextBlob treats text as objects (specifically, a TextBlob object) and provides methods directly on these objects to access common NLP features. It essentially abstracts away the complexity of its underlying libraries (like NLTK) to make common tasks accessible to beginners.\n",
    "\n",
    "\n",
    "How TextBlob Simplifies Common NLP Tasks\n",
    "TextBlob is designed to minimize the code needed to perform analysis.\n",
    "\n",
    "1.  Sentiment Analysis \n",
    "TextBlob includes a pre-trained sentiment analysis model that classifies text based on its expressed emotion. It simplifies this complex task by providing two simple attributes on the TextBlob object: polarity and subjectivity.\n",
    "\n",
    "Polarity: A float ranging from 7$-1.0$ (negative) to 8$+1.0$ (positive).9 The example output of $0.6$ indicates a relatively positive sentiment.\n",
    "\n",
    "Subjectivity: A float ranging from $0.0$ (objective/factual) to $1.0$ (subjective/opinionated). The output of $0.68$ indicates the sentence is fairly opinion-based.\n",
    "\n",
    "2. Translation \n",
    "TextBlob wraps the Google Translate API to provide easy translation functionality without requiring the user to manually handle API keys or complex requests.\n",
    "\n",
    "Question 3: Explain the role of Standford NLP in academic and industry NLP Projects.\n",
    "Answer: Role in Academic Projects\n",
    "In academia, Stanford NLP is a cornerstone for research and benchmarking across the computational linguistics community.\n",
    "\n",
    "Pioneering Research and Publications: The group, led by prominent figures like Christopher Manning, is responsible for groundbreaking research that defines the state of the art in NLP. This research often leads to new models for core tasks, published in top-tier conferences (ACL, EMNLP, etc.), which set the standard for the field.\n",
    "\n",
    "Developing Foundational Models (GloVe): Stanford released GloVe (Global Vectors for Word Representation), a widely used word embedding model. Word embeddings are crucial for nearly all modern NLP tasks, and GloVe became a standard benchmark for comparing the semantic similarity of words.\n",
    "\n",
    "Educational Tools and Resources (CS224n): The group provides comprehensive educational materials, most famously their CS224n (Natural Language Processing with Deep Learning) course, which is freely available online. This curriculum is adopted globally and helps train the next generation of NLP researchers and engineers.\n",
    "\n",
    "Benchmarking Datasets (SNLI): Stanford contributes critical datasets like the Stanford Natural Language Inference (SNLI) corpus, which is used worldwide to train and evaluate models on the task of Natural Language Inference (determining if a sentence implies, contradicts, or is neutral towards another sentence).\n",
    "\n",
    " Role in Industry Projects\n",
    "In industry, the primary contribution of Stanford NLP is its integrated, comprehensive, and high-quality software suite, which provides ready-to-use linguistic analysis tools.\n",
    "\n",
    "1. Stanford CoreNLP (The Integrated Suite)\n",
    "Stanford CoreNLP is the group's most widely adopted tool. It is an integrated suite of Java-based NLP components that can run an entire linguistic analysis pipeline on a text with minimal configuration.\n",
    "\n",
    "Integrated Pipeline: CoreNLP allows users to perform a series of complex tasksâ€”from simple tokenization to advanced analysisâ€”in a single, unified framework. This saves developers significant time and effort in integrating separate tools.\n",
    "\n",
    "Core Tasks Provided:\n",
    "\n",
    "Tokenization and Sentence Splitting\n",
    "\n",
    "Part-of-Speech (POS) Tagging\n",
    "\n",
    "Named Entity Recognition (NER) (identifying people, locations, organizations, dates, etc.)\n",
    "\n",
    "Dependency and Constituency Parsing (determining the grammatical structure of sentences)\n",
    "\n",
    "Coreference Resolution (determining which words, like pronouns, refer to the same entities in a document).\n",
    "\n",
    "Sentiment Analysis\n",
    "\n",
    "Robustness and Accuracy: The algorithms underlying CoreNLP are based on decades of research, making them highly robust and accurate, which is essential for mission-critical applications in finance, healthcare, and legal industries.\n",
    "\n",
    "Multilingual Support: CoreNLP supports multiple languages (including English, Chinese, and Spanish), making it valuable for global businesses and multilingual data processing.\n",
    "\n",
    "2. Stanza (The Python/Neural Toolkit)\n",
    "More recently, Stanford released Stanza, a Python-based NLP package built on neural network models that focuses on providing a full set of core NLP annotations for over 70 languages. This caters to the modern Python-centric data science community and emphasizes speed and accuracy using deep learning methods.\n",
    "\n",
    "3. Specialized Tools\n",
    "Stanford NLP also provides various specialized tools that are frequently incorporated into production systems:\n",
    "\n",
    "Stanford NER: A highly accurate and feature-rich implementation of Named Entity Recognition, often used for information extraction and text summarization.\n",
    "\n",
    "Tregex/Tsurgeon: Tools used for matching and manipulating syntax trees, valuable in both linguistic research and data preprocessing pipelines.\n",
    "\n",
    "\n",
    "Question 4: Describe the architecture and functioning of a Recurrent Natural Network (RNN).  \n",
    "Answer: A Recurrent Neural Network (RNN) is a type of neural network designed to handle sequential data by maintaining a â€œmemoryâ€ of past inputs through hidden states. Unlike feedforward networks, RNNs loop information back into the model, making them powerful for tasks like language modeling, speech recognition, and time-series prediction.\n",
    "\n",
    "Architecture of RNNs\n",
    "Input Layer\n",
    "\n",
    "Accepts sequential data (e.g., words in a sentence, sensor readings over time).\n",
    "\n",
    "Each time step t has an input vector ğ‘¥ğ‘¡.Hidden Layer (Core of RNN)\n",
    "\n",
    "Contains neurons with recurrent connections.\n",
    "\n",
    "At each time step, the hidden state â„ğ‘¡ is updated using both the current input ğ‘¥ğ‘¡ and the previous hidden state â„ğ‘¡âˆ’1.\n",
    "Formula: â„ğ‘¡=ğ‘“(ğ‘Šğ‘¥â„â‹…ğ‘¥ğ‘¡+ğ‘Šâ„â„â‹…â„ğ‘¡âˆ’1+ğ‘â„)where \n",
    "ğ‘“ is an activation function (commonly tanh or ReLU).\n",
    "\n",
    "Output Layer\n",
    "\n",
    "Produces the output ğ‘¦ğ‘¡ at each time step, often using a softmax for classification tasks.\n",
    "\n",
    "Formula:\n",
    "\n",
    "ğ‘¦ğ‘¡=ğ‘”(ğ‘Šâ„ğ‘¦â‹…â„ğ‘¡+ğ‘ğ‘¦)Weights Sharing\n",
    "\n",
    "The same set of weights is reused across all time steps, which makes RNNs efficient and consistent for sequential learning.\n",
    "\n",
    "Functioning of RNNs\n",
    "Sequential Processing\n",
    "\n",
    "Data is fed one element at a time.\n",
    "\n",
    "Each stepâ€™s hidden state depends on both the current input and the previous hidden state, allowing the network to â€œrememberâ€ past information.\n",
    "\n",
    "Memory Mechanism\n",
    "\n",
    "Hidden states act as memory, storing contextual information.\n",
    "\n",
    "This enables RNNs to capture dependencies across time, such as grammar in sentences or trends in time-series data.\n",
    "\n",
    "Training\n",
    "\n",
    "Uses Backpropagation Through Time (BPTT), where errors are propagated backward across multiple time steps.\n",
    "\n",
    "This allows the network to adjust weights based on long-term dependencies, though it can suffer from vanishing/exploding gradients.\n",
    "\n",
    "\n",
    "Question 5: What is the key difference between LSTM and GRU networks in NLP applications?\n",
    "Answer: The key difference between LSTM and GRU networks in NLP is that LSTMs use three gates (input, output, forget) and a separate memory cell to manage long-term dependencies, while GRUs simplify this by using only two gates (reset and update) without a distinct memory cell, making them computationally faster and less complex.\n",
    "\n",
    "Detailed Explanation\n",
    "1. Architecture Differences\n",
    "LSTM (Long Short-Term Memory):\n",
    "\n",
    "Has three gates: input, forget, and output.\n",
    "\n",
    "Maintains a separate cell state in addition to the hidden state.\n",
    "\n",
    "This design allows LSTMs to capture long-term dependencies more effectively, but at the cost of higher computational complexity.\n",
    "\n",
    "GRU (Gated Recurrent Unit):\n",
    "\n",
    "Has two gates: reset and update.\n",
    "\n",
    "Combines the cell state and hidden state into a single representation.\n",
    "\n",
    "This makes GRUs simpler and faster to train, while still handling vanishing gradient problems reasonably well.\n",
    "\n",
    "2. Performance in NLP\n",
    "LSTMs are often preferred when the task requires modeling very long sequences (e.g., document-level context, machine translation).\n",
    "\n",
    "GRUs tend to perform similarly on many NLP tasks (like sentiment analysis, named entity recognition) but with fewer parameters and faster convergence.\n",
    "\n",
    "\n",
    "Question 6: Write a Python program using TextBlob to perform sentiment analysis on\n",
    "the following paragraph of text:\n",
    "â€œI had a great experience using the new mobile banking app. The interface is intuitive,\n",
    "and customer support was quick to resolve my issue. However, the app did crash once\n",
    "during a transaction, which was frustrating\"\n",
    "Your program should print out the polarity and subjectivity scores.\n",
    "(Include your Python code and output in the code box below.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c26cd65-12ab-4823-9658-4e194626fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.21742424242424244\n",
      "Subjectivity: 0.6511363636363636\n"
     ]
    }
   ],
   "source": [
    "# Answer: \n",
    "from textblob import TextBlob\n",
    "\n",
    "# Input text\n",
    "text = \"\"\"I had a great experience using the new mobile banking app. \n",
    "The interface is intuitive, and customer support was quick to resolve my issue. \n",
    "However, the app did crash once during a transaction, which was frustrating\"\"\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiment = blob.sentiment\n",
    "\n",
    "# Print results\n",
    "print(\"Polarity:\", sentiment.polarity)\n",
    "print(\"Subjectivity:\", sentiment.subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ba2fcd-429c-4f52-af4b-3422156888f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\chaur\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\chaur\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\chaur\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\chaur\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chaur\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\chaur\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 624.3/624.3 kB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "136fffac-b49c-4768-b095-3fd050c5914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
      "\n",
      "Frequency Distribution:\n",
      "Natural: 1\n",
      "Language: 1\n",
      "Processing: 1\n",
      "(: 1\n",
      "NLP: 3\n",
      "): 1\n",
      "is: 2\n",
      "a: 1\n",
      "fascinating: 1\n",
      "field: 1\n",
      "that: 1\n",
      "combines: 1\n",
      "linguistics: 1\n",
      ",: 7\n",
      "computer: 1\n",
      "science: 1\n",
      "and: 3\n",
      "artificial: 1\n",
      "intelligence: 1\n",
      ".: 4\n",
      "It: 1\n",
      "enables: 1\n",
      "machines: 1\n",
      "to: 1\n",
      "understand: 1\n",
      "interpret: 1\n",
      "generate: 1\n",
      "human: 1\n",
      "language: 1\n",
      "Applications: 1\n",
      "of: 2\n",
      "include: 1\n",
      "chatbots: 1\n",
      "sentiment: 1\n",
      "analysis: 1\n",
      "machine: 1\n",
      "translation: 1\n",
      "As: 1\n",
      "technology: 1\n",
      "advances: 1\n",
      "the: 1\n",
      "role: 1\n",
      "in: 1\n",
      "modern: 1\n",
      "solutions: 1\n",
      "becoming: 1\n",
      "increasingly: 1\n",
      "critical: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chaur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Question 7: Given the sample paragraph below, perform string tokenization and\n",
    "frequency distribution using Python and NLTK:\n",
    "â€œNatural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
    "computer science, and artificial intelligence. It enables machines to understand,\n",
    "interpret, and generate human language. Applications of NLP include chatbots,\n",
    "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
    "in modern solutions is becoming increasingly critical.â€\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n",
    "\"\"\"\n",
    "#Answer: \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Download required resources (only needed once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample paragraph\n",
    "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
    "computer science, and artificial intelligence. It enables machines to understand,\n",
    "interpret, and generate human language. Applications of NLP include chatbots,\n",
    "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
    "in modern solutions is becoming increasingly critical.\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Frequency distribution\n",
    "freq_dist = FreqDist(tokens)\n",
    "\n",
    "# Print tokens\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "# Print frequency distribution\n",
    "print(\"\\nFrequency Distribution:\")\n",
    "for word, freq in freq_dist.items():\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f283ad41-56b2-4557-b6c2-229cb2cf39a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chaur\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6000 - loss: 0.6932\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4000 - loss: 0.6922\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.6913\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6000 - loss: 0.6903\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6000 - loss: 0.6894\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6000 - loss: 0.6884\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.6000 - loss: 0.6875\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6000 - loss: 0.6865\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6000 - loss: 0.6855\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.6844\n",
      "Training Accuracy: 0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 8: Implement a basic LSTM model in Keras for a text classification task using\n",
    "the following dummy dataset. Your model should classify sentences as either positive\n",
    "(1) or negative (0).\n",
    "# Dataset\n",
    "texts = [\n",
    "â€œI love this projectâ€, #Positive\n",
    "â€œThis is an amazing experienceâ€, #Positive\n",
    "â€œI hate waiting in lineâ€, #Negative\n",
    "â€œThis is the worst serviceâ€, #Negative\n",
    "â€œAbsolutely fantastic!â€ #Positive\n",
    "]\n",
    "labels = [1, 1, 0, 0, 1]\n",
    "Preprocess the text, tokenize it, pad sequences, and build an LSTM model to train on\n",
    "this data. You may use Keras with TensorFlow backend.\n",
    "(Include your Python code and output in the code box below.)\n",
    "\"\"\"\n",
    "\n",
    "#Answer: \n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Dummy dataset\n",
    "texts = [\n",
    "    \"I love this project\",          # Positive\n",
    "    \"This is an amazing experience\",# Positive\n",
    "    \"I hate waiting in line\",       # Negative\n",
    "    \"This is the worst service\",    # Negative\n",
    "    \"Absolutely fantastic!\"         # Positive\n",
    "]\n",
    "labels = [1, 1, 0, 0, 1]\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Step 2: Padding sequences\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "y = np.array(labels)\n",
    "\n",
    "# Step 3: Build LSTM model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=8, input_length=max_len))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(X, y, epochs=10, verbose=1)\n",
    "\n",
    "# Step 5: Evaluate\n",
    "loss, accuracy = model.evaluate(X, y, verbose=0)\n",
    "print(\"Training Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f6b210d-fba2-4dd3-8699-37c45d8d2886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and Lemmas:\n",
      "Homi --> Homi\n",
      "Jehangir --> Jehangir\n",
      "Bhaba --> Bhaba\n",
      "was --> be\n",
      "an --> an\n",
      "Indian --> indian\n",
      "nuclear --> nuclear\n",
      "physicist --> physicist\n",
      "who --> who\n",
      "played --> play\n",
      "a --> a\n",
      "key --> key\n",
      "role --> role\n",
      "in --> in\n",
      "the --> the\n",
      "\n",
      " --> \n",
      "\n",
      "development --> development\n",
      "of --> of\n",
      "India --> India\n",
      "â€™s --> â€™s\n",
      "atomic --> atomic\n",
      "energy --> energy\n",
      "program --> program\n",
      ". --> .\n",
      "He --> he\n",
      "was --> be\n",
      "the --> the\n",
      "founding --> found\n",
      "director --> director\n",
      "of --> of\n",
      "the --> the\n",
      "Tata --> Tata\n",
      "\n",
      " --> \n",
      "\n",
      "Institute --> Institute\n",
      "of --> of\n",
      "Fundamental --> Fundamental\n",
      "Research --> Research\n",
      "( --> (\n",
      "TIFR --> TIFR\n",
      ") --> )\n",
      "and --> and\n",
      "was --> be\n",
      "instrumental --> instrumental\n",
      "in --> in\n",
      "establishing --> establish\n",
      "the --> the\n",
      "\n",
      " --> \n",
      "\n",
      "Atomic --> Atomic\n",
      "Energy --> Energy\n",
      "Commission --> Commission\n",
      "of --> of\n",
      "India --> India\n",
      ". --> .\n",
      "\n",
      "Named Entities:\n",
      "Homi Jehangir Bhaba (FAC)\n",
      "Indian (NORP)\n",
      "India (GPE)\n",
      "the Tata\n",
      "Institute of Fundamental Research (ORG)\n",
      "Atomic Energy Commission of India (ORG)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 9: Using spaCy, build a simple NLP pipeline that includes tokenization,\n",
    "lemmatization, and entity recognition. Use the following paragraph as your dataset:\n",
    "â€œHomi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
    "development of Indiaâ€™s atomic energy program. He was the founding director of the Tata\n",
    "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
    "Atomic Energy Commission of India.â€\n",
    "Write a Python program that processes this text using spaCy, then prints tokens, their\n",
    "lemmas, and any named entities found.\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Answer:\n",
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "text = \"\"\"Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
    "development of Indiaâ€™s atomic energy program. He was the founding director of the Tata\n",
    "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
    "Atomic Energy Commission of India.\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print tokens and their lemmas\n",
    "print(\"Tokens and Lemmas:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} --> {token.lemma_}\")\n",
    "\n",
    "# Print named entities\n",
    "print(\"\\nNamed Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3e701-4001-4b03-9161-9cb3b4a68bee",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Question 10: You are working on a chatbot for a mental health platform. Explain how\n",
    "you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford\n",
    "NLP to understand and respond to user input effectively. Detail your architecture, data\n",
    "preprocessing pipeline, and any ethical considerations.\n",
    "(Include your Python code and output in the code box below.\n",
    "\"\"\"\n",
    "#Answer:  Chatbot architecture for mental health support using LSTM/GRU and spaCy\n",
    "Building an effective, responsible mental health chatbot means combining robust language understanding with careful ethical design. Hereâ€™s a compact architecture that uses LSTM/GRU for intent classification and emotion detection, with spaCy for fast linguistic preprocessing.\n",
    "\n",
    "System overview\n",
    "Input understanding: spaCy for tokenization, lemmatization, and entity recognition; custom keyword patterns for risk phrases (e.g., â€œhurt myself,â€ â€œending itâ€).\n",
    "\n",
    "Core modeling:\n",
    "\n",
    "Intent classifier: LSTM/GRU over word embeddings to categorize messages (e.g., â€œgreeting,â€ â€œseeking resources,â€ â€œcrisis,â€ â€œventing,â€ â€œcoping strategiesâ€).\n",
    "\n",
    "Emotion detector: LSTM/GRU for coarse affect (e.g., â€œsad,â€ â€œanxious,â€ â€œangry,â€ â€œneutralâ€) to guide tone.\n",
    "\n",
    "Response policy:\n",
    "\n",
    "Templates + retrieval snippets (e.g., psychoeducation, grounding techniques).\n",
    "\n",
    "Crisis-safety rule overrides when risk signals are high.\n",
    "\n",
    "Dialogue manager: Confidence-based routing: if intent confidence < threshold, ask clarifying questions; if crisis intent detected, escalate to safe messages and human support guidance.\n",
    "\n",
    "Data preprocessing pipeline\n",
    "Text normalization: Lowercasing, contraction expansion, punctuation trimming.\n",
    "\n",
    "spaCy processing: Tokenization, lemmatization, removal of stopwords when appropriate; retain key entities (PERSON, ORG, GPE) only for context, never for storage beyond session unless user consent is explicit.\n",
    "\n",
    "Vocabulary building: Fit tokenizer on training corpus; handle out-of-vocabulary via UNK token.\n",
    "\n",
    "Padding: Pad/truncate sequences to fixed length for batch training.\n",
    "\n",
    "Labels: Multi-class intents; optional multi-label flags for safety/risk indicators.\n",
    "\n",
    "Ethical considerations\n",
    "Non-diagnostic, non-prescriptive: Provide general information and supportive guidance, never medical advice or medication recommendations. Encourage professional support for concerns.\n",
    "\n",
    "Crisis handling: Prioritize safety messages and suggest reaching out to trusted people or professionals; avoid detailed instructions or links in-model; avoid normalizing harmful behaviors.\n",
    "\n",
    "Privacy and data minimization: Avoid storing sensitive content; anonymize and aggregate for model improvement only with explicit consent.\n",
    "\n",
    "Bias and fairness: Audit training data for stereotypes; avoid identity-based assumptions; keep tone nonjudgmental and inclusive.\n",
    "\n",
    "Transparency and boundaries: Clearly state the chatbot is not a therapist; encourage breaks and human support to reduce dependency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61f84c84-e326-48bb-9092-47f2d222d8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hey, I just wanted to say hi.\n",
      "Predicted intent: uncertain (confidence=0.22)\n",
      "Response: I want to understand better. Could you share a bit more about whatâ€™s going on?\n",
      "\n",
      "Input: I feel anxious and canâ€™t sleep lately.\n",
      "Predicted intent: uncertain (confidence=0.22)\n",
      "Response: I want to understand better. Could you share a bit more about whatâ€™s going on?\n",
      "\n",
      "Input: I think about ending it sometimes.\n",
      "Predicted intent: crisis (confidence=0.22)\n",
      "Response: Iâ€™m really sorry youâ€™re feeling this way. Iâ€™m not a therapist, but your safety matters. Please consider reaching out to someone you trust or a professional for immediate support.\n",
      "\n",
      "Input: Where can I get help near me?\n",
      "Predicted intent: uncertain (confidence=0.22)\n",
      "Response: I want to understand better. Could you share a bit more about whatâ€™s going on?\n",
      "\n",
      "Input: What can I do to calm down during panic?\n",
      "Predicted intent: uncertain (confidence=0.22)\n",
      "Response: I want to understand better. Could you share a bit more about whatâ€™s going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Minimal demonstration: spaCy preprocessing + GRU intent classifier + simple response policy\n",
    "# Note: This is a toy example with dummy data for illustration. In production, use larger,\n",
    "#       curated datasets, evaluation, calibration, and robust safety tooling.\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# -----------------------\n",
    "# 1) Load spaCy pipeline\n",
    "# -----------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Dummy training data\n",
    "#    Intents: 0=greeting, 1=venting, 2=coping_support, 3=seeking_resources, 4=crisis\n",
    "# -----------------------\n",
    "texts = [\n",
    "    \"Hi there, I just wanted to say hello\",\n",
    "    \"I'm feeling really overwhelmed and sad today\",\n",
    "    \"Can you share some grounding techniques for anxiety?\",\n",
    "    \"Where can I find mental health resources near me?\",\n",
    "    \"I don't want to go on anymore, I might hurt myself\",\n",
    "    \"I feel angry about what happened at work\",\n",
    "    \"How do I cope with stress during exams?\",\n",
    "    \"Hey, good to see you\",\n",
    "    \"I feel anxious and can't sleep\",\n",
    "    \"I think about ending it sometimes\"\n",
    "]\n",
    "labels = [0, 1, 2, 3, 4, 1, 2, 0, 1, 4]\n",
    "num_classes = 5\n",
    "\n",
    "# -----------------------\n",
    "# 3) Preprocessing helpers\n",
    "# -----------------------\n",
    "def normalize_text(t):\n",
    "    t = t.lower().strip()\n",
    "    t = re.sub(r\"[^a-z0-9\\s']\", \" \", t)\n",
    "    # basic contraction handling examples\n",
    "    t = t.replace(\"can't\", \"can not\").replace(\"don't\", \"do not\").replace(\"won't\", \"will not\")\n",
    "    return t\n",
    "\n",
    "def spacy_lemmatize(t):\n",
    "    doc = nlp(t)\n",
    "    # Keep pronouns and negations, remove most stopwords but retain key ones\n",
    "    tokens = []\n",
    "    for tok in doc:\n",
    "        if tok.is_punct or tok.like_num:\n",
    "            continue\n",
    "        # keep negations and important words\n",
    "        if tok.is_stop and tok.lemma_ not in [\"not\", \"no\"]:\n",
    "            continue\n",
    "        tokens.append(tok.lemma_)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "preprocessed = [spacy_lemmatize(normalize_text(t)) for t in texts]\n",
    "\n",
    "# -----------------------\n",
    "# 4) Tokenization & padding\n",
    "# -----------------------\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(preprocessed)\n",
    "seqs = tokenizer.texts_to_sequences(preprocessed)\n",
    "max_len = max(len(s) for s in seqs)\n",
    "X = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "y = to_categorical(np.array(labels), num_classes=num_classes)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# -----------------------\n",
    "# 5) Build GRU intent classifier\n",
    "# -----------------------\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=16, input_length=max_len),\n",
    "    GRU(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, epochs=15, verbose=0)\n",
    "\n",
    "# -----------------------\n",
    "# 6) Simple rule-enhanced response policy\n",
    "# -----------------------\n",
    "RISK_PATTERNS = [\n",
    "    r\"\\bhurt myself\\b\", r\"\\bend(ing)? it\\b\", r\"\\bsuicide\\b\", r\"\\bkill myself\\b\",\n",
    "    r\"\\bdo not want to go on\\b\", r\"\\bno reason to live\\b\"\n",
    "]\n",
    "\n",
    "def detect_risk(raw_text):\n",
    "    txt = raw_text.lower()\n",
    "    for pat in RISK_PATTERNS:\n",
    "        if re.search(pat, txt):\n",
    "            return True\n",
    "    # spaCy NER or dependency could add nuance; here we keep it simple\n",
    "    return False\n",
    "\n",
    "INTENT_TEMPLATES = {\n",
    "    0: \"Hi. Iâ€™m here to listen. How are you feeling right now?\",\n",
    "    1: \"Thanks for sharing. That sounds heavy. If youâ€™d like, we can unpack whatâ€™s feeling most intense.\",\n",
    "    2: \"Here are a few general coping ideas: slow breathing, naming five things you see, and brief journaling.\",\n",
    "    3: \"I can share general information about support options. Would you like guidance on finding local professionals?\",\n",
    "    4: \"Iâ€™m really sorry youâ€™re feeling this way. Iâ€™m not a therapist, but your safety matters. Please consider reaching out to someone you trust or a professional for immediate support.\"\n",
    "}\n",
    "\n",
    "def respond(raw_text):\n",
    "    # preprocess\n",
    "    p = spacy_lemmatize(normalize_text(raw_text))\n",
    "    seq = tokenizer.texts_to_sequences([p])\n",
    "    Xq = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "    probs = model.predict(Xq, verbose=0)[0]\n",
    "    intent = int(np.argmax(probs))\n",
    "    confidence = float(np.max(probs))\n",
    "\n",
    "    # safety override\n",
    "    if detect_risk(raw_text) or intent == 4:\n",
    "        return {\n",
    "            \"intent\": \"crisis\",\n",
    "            \"confidence\": confidence,\n",
    "            \"response\": INTENT_TEMPLATES[4]\n",
    "        }\n",
    "\n",
    "    # low confidence -> clarify\n",
    "    if confidence < 0.50:\n",
    "        return {\n",
    "            \"intent\": \"uncertain\",\n",
    "            \"confidence\": confidence,\n",
    "            \"response\": \"I want to understand better. Could you share a bit more about whatâ€™s going on?\"\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"intent\": [\"greeting\", \"venting\", \"coping_support\", \"seeking_resources\", \"crisis\"][intent],\n",
    "        \"confidence\": confidence,\n",
    "        \"response\": INTENT_TEMPLATES[intent]\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "# 7) Demo queries\n",
    "# -----------------------\n",
    "samples = [\n",
    "    \"Hey, I just wanted to say hi.\",\n",
    "    \"I feel anxious and canâ€™t sleep lately.\",\n",
    "    \"I think about ending it sometimes.\",\n",
    "    \"Where can I get help near me?\",\n",
    "    \"What can I do to calm down during panic?\"\n",
    "]\n",
    "\n",
    "for s in samples:\n",
    "    out = respond(s)\n",
    "    print(f\"Input: {s}\")\n",
    "    print(f\"Predicted intent: {out['intent']} (confidence={out['confidence']:.2f})\")\n",
    "    print(f\"Response: {out['response']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f729f-a206-467e-a42b-51f1e0b9f40b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
